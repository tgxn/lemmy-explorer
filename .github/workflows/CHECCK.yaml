name: publish-pages

on:
  # Runs on pushes targeting the default branch
  push:
    branches:
      - backport_rdb

env:
  NODE_VERSION: 22.17.0

jobs:
  # download the redis dump from s3
  # create the json files for the redis data
  # upload the json files to github artifacts
  publish_pages:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      # https://github.com/actions/toolkit/issues/946#issuecomment-1590016041
      - name: root suid tar
        run: sudo chown root:root /bin/tar && sudo chmod u+s /bin/tar

      - uses: actions/checkout@v4

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Use Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: get current hour for cache busting
        id: cache-hour
        run: echo "hour=$(date +'%Y-%m-%d-%H')" >>$GITHUB_OUTPUT

      - name: Cache Redis Dump
        id: cache-redis
        uses: actions/cache@v4
        env:
          cache-name: cache-redis
        with:
          path: ./.redis/dump.rdb
          key: cache-redis-${{ steps.cache-hour.outputs.hour }}

      # download redis db dump from s3
      - name: Download the Redis Dump
        if: steps.cache-redis.outputs.cache-hit != 'true'
        uses: keithweaver/aws-s3-github-action@v1.0.0
        with:
          command: cp
          source: s3://${{ vars.BUILD_S3_BUCKET }}/checkpoint/dump.rdb
          destination: ./.redis/dump.rdb
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws_region: ap-southeast-2

      # set compose_command envvar
      - name: set global workflow ennvvar
        run: echo "COMPOSE_COMMAND=docker compose -f docker-compose.github.yaml" >> $GITHUB_ENV

      # start redis & check if it is running
      - name: Start Redis
        working-directory: ./crawler
        run: ${{ env.COMPOSE_COMMAND }} up -d redis

      - name: Wait for Redis to be ready
        working-directory: ./crawler
        run: |
          until ${{ env.COMPOSE_COMMAND }} exec redis redis-cli ping | grep PONG; do
            ${{ env.COMPOSE_COMMAND }} logs -n 100 redis

            echo "Waiting for Redis..."
            sleep 1
          done

      - working-directory: ./crawler
        run: docker ps -a
      - working-directory: ./crawler
        run: ${{ env.COMPOSE_COMMAND }} logs redis

      # Install + Cache Crawler Dependencies
      - name: Cache Node Modules | Crawler
        id: cache-crawler-yarn
        uses: actions/cache@v4
        env:
          cache-name: cache-crawler-yarn
        with:
          path: ./crawler/node_modules/
          key: cache-crawler-yarn-${{ hashFiles('crawler/yarn.lock') }}

      - name: Install Node Modules | Crawler
        if: steps.cache-crawler-yarn.outputs.cache-hit != 'true'
        run: yarn --frozen-lockfile
        working-directory: ./crawler

      # Run Crawler Output Script
      - name: Run Health Script
        run: yarn health
        working-directory: ./crawler

      - name: Run Output Script
        run: yarn output
        working-directory: ./crawler

      # Create ZIP Archive of JSON Files in ./frontend/public/data/
      - name: Create ZIP Archive of JSON Files
        working-directory: ./frontend/public/data
        run: |
          zip -r -9 -q json-bundle.zip ./*
          du -sh json-bundle.zip

      - name: copy the json files to github pages
        working-directory: ./
        run: cp -r ./frontend/public/data/ ./pages/public/

      - name: copy the raw db dump file to github pages
        working-directory: ./
        run: cp -r ./.redis/dump.rdb ./pages/public/data/lemmyverse.rdb
